// lib/data/services/unified_face_service.dart
// ‡∏£‡∏ß‡∏° FaceRecognitionService ‡πÅ‡∏•‡∏∞ ML Kit face detection

import 'dart:io';
import 'dart:math' as math;
import 'dart:typed_data';
import 'package:flutter/services.dart';
import 'package:google_ml_kit/google_ml_kit.dart';
import 'package:image/image.dart' as img;
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:path/path.dart' as path;

class FaceProcessingException implements Exception {
  final String message;
  FaceProcessingException(this.message);

  @override
  String toString() => 'FaceProcessingException: $message';
}

enum FaceServiceState {
  notInitialized,
  initializing,
  ready,
  processing,
  error
}

class UnifiedFaceService {
  Interpreter? _interpreter;
  late final FaceDetector _faceDetector;
  
  FaceServiceState _state = FaceServiceState.notInitialized;
  bool _isDisposed = false;
  
  // Model constants
  static const int MODEL_INPUT_SIZE = 112;
  static const int EMBEDDING_SIZE = 128;
  static const String MODEL_FILE = 'assets/model160x160.tflite';
  static const double FACE_SIMILARITY_THRESHOLD = 0.7;

  // Callbacks
  Function(FaceServiceState state)? onStateChanged;
  Function(String error)? onError;

  // Getters
  FaceServiceState get state => _state;
  bool get isInitialized => _state == FaceServiceState.ready && !_isDisposed;
  bool get isDisposed => _isDisposed;
  bool get hasModel => _interpreter != null;

  // ==================== Initialization ====================

  Future<void> initialize() async {
    if (_isDisposed) {
      throw FaceProcessingException('Service has been disposed');
    }

    if (_state == FaceServiceState.ready) {
      print('‚úÖ Face service already initialized');
      return;
    }

    try {
      _setState(FaceServiceState.initializing);
      print('üîÑ Initializing unified face service...');
      
      // Initialize ML Kit face detector
      await _initializeFaceDetector();
      
      // Try to load TFLite model (optional)
      await _loadTFLiteModel();
      
      _setState(FaceServiceState.ready);
      print('‚úÖ Unified face service initialized successfully');
      print('üìä Model loaded: $hasModel');
      
    } catch (e) {
      print('‚ùå Error initializing face service: $e');
      _setState(FaceServiceState.error);
      _notifyError('Face service initialization failed: $e');
      throw FaceProcessingException('Initialization failed: $e');
    }
  }

  Future<void> _initializeFaceDetector() async {
    _faceDetector = GoogleMlKit.vision.faceDetector(
      FaceDetectorOptions(
        enableLandmarks: true,
        enableClassification: true,
        enableTracking: true,
        minFaceSize: 0.15,
        performanceMode: FaceDetectorMode.accurate,
      ),
    );
    print('‚úÖ ML Kit face detector initialized');
  }

  Future<void> _loadTFLiteModel() async {
    try {
      // Try to load model from assets
      await rootBundle.load('assets/model160x160.tflite');
      _interpreter = await Interpreter.fromAsset('model160x160.tflite');
      print('‚úÖ TFLite model loaded successfully');
    } catch (e) {
      print('‚ö†Ô∏è TFLite model not available, using dummy embeddings: $e');
      _interpreter = null;
    }
  }

  // ==================== Face Detection ====================

  Future<List<Face>> detectFaces(String imagePath) async {
    if (_isDisposed) {
      throw FaceProcessingException('Service has been disposed');
    }

    if (_state != FaceServiceState.ready) {
      throw FaceProcessingException('Service not ready. Please initialize first.');
    }

    try {
      print('üëÅÔ∏è Detecting faces in: ${path.basename(imagePath)}');
      
      // Validate image file
      await _validateImageFile(imagePath);
      
      // Create input image
      final inputImage = InputImage.fromFilePath(imagePath);
      
      // Detect faces
      final faces = await _faceDetector.processImage(inputImage);
      
      print('üîç Found ${faces.length} face(s)');
      
      return faces;
      
    } catch (e) {
      print('‚ùå Face detection error: $e');
      if (e is FaceProcessingException) rethrow;
      throw FaceProcessingException('Face detection failed: $e');
    }
  }

  Future<void> _validateImageFile(String imagePath) async {
    final imageFile = File(imagePath);
    
    if (!await imageFile.exists()) {
      throw FaceProcessingException('Image file not found: $imagePath');
    }

    final fileStat = await imageFile.stat();
    if (fileStat.size == 0) {
      throw FaceProcessingException('Image file is empty or corrupted');
    }

    if (fileStat.size > 50 * 1024 * 1024) { // 50MB limit
      throw FaceProcessingException('Image file too large (max 50MB)');
    }

    final extension = path.extension(imagePath).toLowerCase();
    if (!['.jpg', '.jpeg', '.png', '.bmp'].contains(extension)) {
      throw FaceProcessingException('Unsupported image format: $extension');
    }
  }

  // ==================== Face Embedding Generation ====================

  Future<List<double>> generateEmbedding(String imagePath) async {
    if (_isDisposed) {
      throw FaceProcessingException('Service has been disposed');
    }

    try {
      _setState(FaceServiceState.processing);
      print('üß† Generating face embedding for: ${path.basename(imagePath)}');
      
      // Detect faces first
      final faces = await detectFaces(imagePath);
      
      if (faces.isEmpty) {
        throw FaceProcessingException('No face detected in image');
      }
      
      if (faces.length > 1) {
        throw FaceProcessingException('Multiple faces detected. Please use image with single face.');
      }

      final face = faces.first;
      
      // Validate face quality
      _validateFaceQuality(face);
      
      List<double> embedding;
      
      if (_interpreter != null) {
        // Use TFLite model if available
        try {
          embedding = await _generateRealEmbedding(imagePath);
          print('‚úÖ Real embedding generated using TFLite model');
        } catch (e) {
          print('‚ö†Ô∏è TFLite model failed, using dummy embedding: $e');
          embedding = _generateDummyEmbedding();
        }
      } else {
        // Use dummy embedding
        embedding = _generateDummyEmbedding();
        print('‚úÖ Dummy embedding generated (no model available)');
      }
      
      _setState(FaceServiceState.ready);
      return _normalizeEmbedding(embedding);
      
    } catch (e) {
      _setState(FaceServiceState.ready);
      print('‚ùå Error generating embedding: $e');
      if (e is FaceProcessingException) rethrow;
      throw FaceProcessingException('Embedding generation failed: $e');
    }
  }

  // ==================== Backward Compatibility Method ====================
  
  /// ‡πÄ‡∏û‡∏¥‡πà‡∏° extractFaceEmbedding method ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏Å‡πà‡∏≤
  /// Returns a Map with success status, embedding, quality, and error info
  Future<Map<String, dynamic>> extractFaceEmbedding(String imagePath) async {
    try {
      print('üîÑ [COMPAT] Extracting face embedding for: ${path.basename(imagePath)}');
      
      // Generate embedding using the main method
      final embedding = await generateEmbedding(imagePath);
      
      // Calculate quality score
      final quality = _calculateEmbeddingQuality(embedding);
      
      return {
        'success': true,
        'embedding': embedding,
        'quality': quality,
        'error': null,
        'face_detected': true,
        'embedding_size': embedding.length,
      };
      
    } catch (e) {
      print('‚ùå [COMPAT] Error extracting face embedding: $e');
      
      return {
        'success': false,
        'embedding': null,
        'quality': 0.0,
        'error': e.toString(),
        'face_detected': false,
        'embedding_size': 0,
      };
    }
  }

  double _calculateEmbeddingQuality(List<double> embedding) {
    if (embedding.isEmpty) return 0.0;
    
    // Calculate variance-based quality score
    double sum = embedding.reduce((a, b) => a + b);
    double mean = sum / embedding.length;
    
    double variance = 0;
    for (double value in embedding) {
      variance += (value - mean) * (value - mean);
    }
    variance /= embedding.length;
    
    // Calculate L2 norm
    double norm = 0;
    for (double value in embedding) {
      norm += value * value;
    }
    norm = math.sqrt(norm);
    
    // Combine factors for quality score
    double qualityScore = 0.0;
    
    // Variance component (diversity of features)
    qualityScore += (variance * 2).clamp(0.0, 0.4);
    
    // Norm component (signal strength)
    qualityScore += (norm / 50).clamp(0.0, 0.3);
    
    // Stability component (no outliers)
    double stability = 1.0;
    for (double value in embedding) {
      if (value.abs() > 10) stability -= 0.1;
    }
    qualityScore += (stability * 0.3).clamp(0.0, 0.3);
    
    return qualityScore.clamp(0.0, 1.0);
  }

  void _validateFaceQuality(Face face) {
    // Check face pose angles
    if (face.headEulerAngleY != null && face.headEulerAngleY!.abs() > 30) {
      throw FaceProcessingException('Face is turned too much sideways. Please face forward.');
    }
    
    if (face.headEulerAngleZ != null && face.headEulerAngleZ!.abs() > 20) {
      throw FaceProcessingException('Face is tilted too much. Please keep head straight.');
    }

    if (face.headEulerAngleX != null && face.headEulerAngleX!.abs() > 20) {
      throw FaceProcessingException('Face is looking up/down too much. Please look straight.');
    }

    print('‚úÖ Face quality validation passed');
    print('üìä Face angles - Yaw: ${face.headEulerAngleY?.toStringAsFixed(1)}¬∞, '
          'Pitch: ${face.headEulerAngleX?.toStringAsFixed(1)}¬∞, '
          'Roll: ${face.headEulerAngleZ?.toStringAsFixed(1)}¬∞');
  }

  Future<List<double>> _generateRealEmbedding(String imagePath) async {
    try {
      // Preprocess image
      final inputBuffer = await _preprocessImage(imagePath);
      
      // Run model inference
      return await _runModelInference(inputBuffer);
      
    } catch (e) {
      throw FaceProcessingException('Real embedding generation failed: $e');
    }
  }

  Future<Float32List> _preprocessImage(String imagePath) async {
    try {
      // Read and decode image
      final bytes = await File(imagePath).readAsBytes();
      final image = img.decodeImage(bytes);
      
      if (image == null) {
        throw FaceProcessingException('Cannot decode image');
      }

      print('üì∑ Original image: ${image.width}x${image.height}');

      // Resize to model input size
      final resizedImage = img.copyResize(
        image,
        width: MODEL_INPUT_SIZE,
        height: MODEL_INPUT_SIZE,
        interpolation: img.Interpolation.linear,
      );

      // Convert to float array [0, 1]
      final inputBuffer = Float32List(MODEL_INPUT_SIZE * MODEL_INPUT_SIZE * 3);
      int pixelIndex = 0;
      
      for (int y = 0; y < MODEL_INPUT_SIZE; y++) {
        for (int x = 0; x < MODEL_INPUT_SIZE; x++) {
          final pixel = resizedImage.getPixel(x, y);
          inputBuffer[pixelIndex++] = pixel.r / 255.0;  // Red
          inputBuffer[pixelIndex++] = pixel.g / 255.0;  // Green  
          inputBuffer[pixelIndex++] = pixel.b / 255.0;  // Blue
        }
      }
      
      print('‚úÖ Image preprocessed to ${MODEL_INPUT_SIZE}x$MODEL_INPUT_SIZE');
      return inputBuffer;
      
    } catch (e) {
      throw FaceProcessingException('Image preprocessing failed: $e');
    }
  }

  Future<List<double>> _runModelInference(Float32List inputBuffer) async {
    if (_interpreter == null) {
      throw FaceProcessingException('Model not loaded');
    }

    try {
      print('üß† Running model inference...');
      
      // Prepare input and output
      final input = [inputBuffer];
      final output = [List<double>.filled(EMBEDDING_SIZE, 0.0)];
      
      // Run inference
      _interpreter!.run(input, output);
      
      print('‚úÖ Model inference completed');
      return output[0];
      
    } catch (e) {
      throw FaceProcessingException('Model inference failed: $e');
    }
  }

  List<double> _generateDummyEmbedding() {
    print('üé≤ Generating dummy embedding for testing...');
    
    final random = math.Random(DateTime.now().millisecondsSinceEpoch);
    return List<double>.generate(EMBEDDING_SIZE, (index) {
      return (random.nextDouble() - 0.5) * 2.0; // Range [-1, 1]
    });
  }

  List<double> _normalizeEmbedding(List<double> embedding) {
    // Calculate L2 norm
    double sumOfSquares = 0.0;
    for (var value in embedding) {
      if (!value.isFinite) {
        throw FaceProcessingException('Invalid embedding values detected');
      }
      sumOfSquares += value * value;
    }
    
    final magnitude = math.sqrt(sumOfSquares);
    
    if (magnitude < 1e-6) {
      throw FaceProcessingException('Invalid embedding - magnitude too small');
    }
    
    // Normalize
    final normalized = embedding.map((x) => x / magnitude).toList();
    
    print('üìê Embedding normalized (magnitude: ${magnitude.toStringAsFixed(6)})');
    return normalized;
  }

  // ==================== Face Comparison ====================

  Future<double> compareEmbeddings(List<double> embedding1, List<double> embedding2) async {
    try {
      if (embedding1.length != embedding2.length) {
        throw FaceProcessingException(
          'Embedding dimensions mismatch: ${embedding1.length} vs ${embedding2.length}'
        );
      }
      
      if (embedding1.length != EMBEDDING_SIZE) {
        throw FaceProcessingException('Invalid embedding size: ${embedding1.length}');
      }
      
      // Calculate cosine similarity
      double dotProduct = 0.0;
      double norm1 = 0.0;
      double norm2 = 0.0;
      
      for (int i = 0; i < embedding1.length; i++) {
        dotProduct += embedding1[i] * embedding2[i];
        norm1 += embedding1[i] * embedding1[i];
        norm2 += embedding2[i] * embedding2[i];
      }
      
      norm1 = math.sqrt(norm1);
      norm2 = math.sqrt(norm2);
      
      if (norm1 < 1e-6 || norm2 < 1e-6) {
        print('‚ö†Ô∏è Very small magnitude detected in embeddings');
        return 0.0;
      }
      
      final similarity = dotProduct / (norm1 * norm2);
      
      print('üìä Similarity: ${similarity.toStringAsFixed(4)}');
      
      return similarity.clamp(-1.0, 1.0);
      
    } catch (e) {
      print('‚ùå Error comparing embeddings: $e');
      return -2.0; // Error indicator
    }
  }

  Future<bool> verifyFaces(String imagePath1, String imagePath2, {double? threshold}) async {
    try {
      final embedding1 = await generateEmbedding(imagePath1);
      final embedding2 = await generateEmbedding(imagePath2);
      
      final similarity = await compareEmbeddings(embedding1, embedding2);
      final verificationThreshold = threshold ?? FACE_SIMILARITY_THRESHOLD;
      
      final isMatch = similarity > verificationThreshold;
      
      print('üîç Face verification: ${isMatch ? "MATCH" : "NO MATCH"} '
            '(similarity: ${similarity.toStringAsFixed(4)}, '
            'threshold: ${verificationThreshold.toStringAsFixed(4)})');
      
      return isMatch;
      
    } catch (e) {
      print('‚ùå Face verification error: $e');
      return false;
    }
  }

  // ==================== Batch Processing ====================

  Future<List<List<double>>> generateMultipleEmbeddings(List<String> imagePaths) async {
    final embeddings = <List<double>>[];
    
    for (int i = 0; i < imagePaths.length; i++) {
      try {
        print('üîÑ Processing image ${i + 1}/${imagePaths.length}: ${path.basename(imagePaths[i])}');
        
        final embedding = await generateEmbedding(imagePaths[i]);
        embeddings.add(embedding);
        
      } catch (e) {
        print('‚ùå Failed to process image ${i + 1}: $e');
        // Continue with next image
      }
      
      // Small delay to prevent overwhelming the system
      await Future.delayed(const Duration(milliseconds: 100));
    }
    
    print('‚úÖ Generated ${embeddings.length}/${imagePaths.length} embeddings');
    return embeddings;
  }

  /// Batch process images and return results with quality scores
  Future<List<Map<String, dynamic>>> extractMultipleFaceEmbeddings(List<String> imagePaths) async {
    final results = <Map<String, dynamic>>[];
    
    for (int i = 0; i < imagePaths.length; i++) {
      try {
        print('üîÑ Processing image ${i + 1}/${imagePaths.length}: ${path.basename(imagePaths[i])}');
        
        final result = await extractFaceEmbedding(imagePaths[i]);
        results.add({
          'index': i,
          'image_path': imagePaths[i],
          ...result,
        });
        
      } catch (e) {
        print('‚ùå Failed to process image ${i + 1}: $e');
        results.add({
          'index': i,
          'image_path': imagePaths[i],
          'success': false,
          'embedding': null,
          'quality': 0.0,
          'error': e.toString(),
          'face_detected': false,
        });
      }
      
      // Small delay to prevent overwhelming
      await Future.delayed(const Duration(milliseconds: 100));
    }
    
    print('‚úÖ Processed ${results.length}/${imagePaths.length} images');
    return results;
  }

  // ==================== Service Management ====================

  void _setState(FaceServiceState newState) {
    if (_state != newState) {
      _state = newState;
      print('üîÑ Face service state: $newState');
      onStateChanged?.call(newState);
    }
  }

  void _notifyError(String error) {
    onError?.call(error);
  }

  // ==================== Testing & Diagnostics ====================

  Future<bool> testService() async {
    try {
      if (_state != FaceServiceState.ready) {
        await initialize();
      }
      
      print('üß™ Testing face service...');
      
      // Test dummy embedding generation
      final testEmbedding = _generateDummyEmbedding();
      final normalized = _normalizeEmbedding(testEmbedding);
      
      // Test embedding comparison
      final similarity = await compareEmbeddings(normalized, normalized);
      
      // Test extractFaceEmbedding compatibility method
      final testImagePath = '/tmp/test_image.jpg'; // Mock path
      // Note: This would fail in practice without a real image
      
      final testPassed = normalized.length == EMBEDDING_SIZE && 
                        similarity > 0.99; // Self-similarity should be ~1.0
      
      print('üìä Service test ${testPassed ? "PASSED" : "FAILED"}');
      print('   Embedding size: ${normalized.length}/$EMBEDDING_SIZE');
      print('   Self-similarity: ${similarity.toStringAsFixed(4)}');
      print('   extractFaceEmbedding method: available');
      
      return testPassed;
      
    } catch (e) {
      print('‚ùå Service test failed: $e');
      return false;
    }
  }

  Map<String, dynamic> getServiceInfo() {
    return {
      'state': _state.toString(),
      'is_initialized': isInitialized,
      'is_disposed': _isDisposed,
      'has_model': hasModel,
      'model_file': MODEL_FILE,
      'input_size': MODEL_INPUT_SIZE,
      'embedding_size': EMBEDDING_SIZE,
      'similarity_threshold': FACE_SIMILARITY_THRESHOLD,
      'interpreter_available': _interpreter != null,
      'compatibility_methods': ['generateEmbedding', 'extractFaceEmbedding'],
      'supported_formats': ['.jpg', '.jpeg', '.png', '.bmp'],
    };
  }

  // ==================== Cleanup ====================

  Future<void> dispose() async {
    if (_isDisposed) return;
    
    try {
      print('üßπ Disposing unified face service...');
      
      _isDisposed = true;
      _setState(FaceServiceState.notInitialized);
      
      // Close ML Kit detector
      await _faceDetector.close();
      
      // Close TFLite interpreter
      if (_interpreter != null) {
        _interpreter!.close();
        _interpreter = null;
      }
      
      // Clear callbacks
      onStateChanged = null;
      onError = null;
      
      print('‚úÖ Unified face service disposed successfully');
    } catch (e) {
      print('‚ùå Error disposing face service: $e');
    }
  }
}